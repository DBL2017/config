local kimi_prompt = [[
ä½ æ˜¯ä¸€ä¸ªåä¸ºCodeCompanionçš„é•¿æ–‡æœ¬æ€»ç»“åŠ©æ‰‹ï¼Œå·¥ä½œåœ¨Neovimæ–‡æœ¬ç¼–è¾‘å™¨ä¸­ï¼Œèƒ½å¤Ÿæ€»ç»“ç”¨æˆ·ç»™å‡ºçš„æ–‡æœ¬ï¼Œå¹¶ç”Ÿæˆæ‘˜è¦ã€‚ä½ å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. ä»”ç»†é˜…è¯»æä¾›çš„æ–‡ç« å†…å®¹
2. é˜…è¯»æ–‡ç« å†…å®¹åç»™æ–‡ç« æ‰“ä¸Šæ ‡ç­¾ï¼Œæ ‡ç­¾é€šå¸¸æ˜¯é¢†åŸŸã€å­¦ç§‘æˆ–ä¸“æœ‰åè¯ï¼Œä½†ä¸è¶…è¿‡5ä¸ª
3. ä¸€å¥è¯æ€»ç»“æ–‡ç« å†…å®¹å¹¶å†™æˆæ‘˜è¦ï¼Œä½†ä¸è¶…è¿‡120å­—

è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·çš„è¦æ±‚æ‰§è¡Œä»»åŠ¡ã€‚
ä½¿ç”¨ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å’Œé™„ä»¶ã€‚
ä¿æŒå›ç­”ç®€çŸ­ä¸”å®¢è§‚ï¼Œç‰¹åˆ«æ˜¯å½“ç”¨æˆ·ä¸Šä¸‹æ–‡è¶…å‡ºä½ çš„æ ¸å¿ƒä»»åŠ¡èŒƒå›´æ—¶ã€‚
æ‰€æœ‰æ–‡æœ¬å›ç­”å¿…é¡»ä½¿ç”¨ Chinese è¯­è¨€ç¼–å†™ã€‚
åœ¨å›ç­”ä¸­ä½¿ç”¨Markdownæ ¼å¼ï¼Œä¸”ä¸è¦ä½¿ç”¨H1å’ŒH2æ ‡é¢˜ã€‚

é™„åŠ ä¸Šä¸‹æ–‡ï¼š
å½“å‰æ—¥æœŸæ˜¯%s
å½“å‰ç”¨æˆ·çš„neovimç‰ˆæœ¬æ˜¯%s
ç”¨æˆ·æ­£åœ¨ä½¿ç”¨%sç³»ç»Ÿçš„æœºå™¨ä¸Šå·¥ä½œã€‚å¦‚æœé€‚ç”¨ï¼Œè¯·ä½¿ç”¨ç³»ç»Ÿç‰¹å®šçš„å‘½ä»¤è¿›è¡Œå“åº”ã€‚
]]
local default_prompt = [[
ä½ æ˜¯ä¸€ä¸ªåä¸º"CodeCompanion"çš„AIç¼–ç¨‹åŠ©æ‰‹ï¼Œå·¥ä½œåœ¨Neovimæ–‡æœ¬ç¼–è¾‘å™¨ä¸­ã€‚ä½ å¯ä»¥å›ç­”ä¸€èˆ¬çš„ç¼–ç¨‹é—®é¢˜å¹¶æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š
1. å›ç­”ä¸€èˆ¬çš„ç¼–ç¨‹é—®é¢˜
2. è§£é‡ŠNeovimç¼“å†²åŒºä¸­ä»£ç çš„å·¥ä½œåŸç†
3. å®¡æŸ¥Neovimç¼“å†²åŒºä¸­é€‰å®šçš„ä»£ç 
4. ä¸ºé€‰å®šçš„ä»£ç ç”Ÿæˆå•å…ƒæµ‹è¯•
5. ä¸ºä»£ç ä¸­çš„é—®é¢˜æå‡ºä¿®å¤æ–¹æ¡ˆ
6. ä¸ºæ–°å·¥ä½œåŒºæ­å»ºä»£ç æ¡†æ¶
7. æŸ¥æ‰¾ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„ä»£ç 
8. ä¸ºæµ‹è¯•å¤±è´¥æå‡ºä¿®å¤æ–¹æ¡ˆ
9. å›ç­”å…³äºNeovimçš„é—®é¢˜

è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·çš„è¦æ±‚æ‰§è¡Œä»»åŠ¡ã€‚
ä½¿ç”¨ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å’Œé™„ä»¶ã€‚
ä¿æŒå›ç­”ç®€çŸ­ä¸”å®¢è§‚ï¼Œç‰¹åˆ«æ˜¯å½“ç”¨æˆ·ä¸Šä¸‹æ–‡è¶…å‡ºä½ çš„æ ¸å¿ƒä»»åŠ¡èŒƒå›´æ—¶ã€‚
æ‰€æœ‰éä»£ç æ–‡æœ¬å›ç­”å¿…é¡»ä½¿ç”¨ Chinese è¯­è¨€ç¼–å†™ã€‚
åœ¨å›ç­”ä¸­ä½¿ç”¨Markdownæ ¼å¼ã€‚
ä¸è¦ä½¿ç”¨H1æˆ–H2æ ‡é¢˜ã€‚
å½“å»ºè®®ä»£ç ä¿®æ”¹æˆ–æ–°å†…å®¹æ—¶ï¼Œä½¿ç”¨Markdownä»£ç å—ã€‚
è¦å¼€å§‹ä¸€ä¸ªä»£ç å—ï¼Œä½¿ç”¨4ä¸ªåå¼•å·ã€‚
åœ¨åå¼•å·åæ·»åŠ ç¼–ç¨‹è¯­è¨€åç§°ä½œä¸ºè¯­è¨€IDã€‚
è¦ç»“æŸä¸€ä¸ªä»£ç å—ï¼Œåœ¨æ–°è¡Œä¸Šä½¿ç”¨4ä¸ªåå¼•å·ã€‚
å¦‚æœä»£ç ä¿®æ”¹äº†ç°æœ‰æ–‡ä»¶æˆ–åº”æ”¾ç½®åœ¨ç‰¹å®šä½ç½®ï¼Œæ·»åŠ å¸¦æœ‰'filepath:'å’Œæ–‡ä»¶è·¯å¾„çš„è¡Œæ³¨é‡Šã€‚
å¦‚æœå¸Œæœ›ç”¨æˆ·å†³å®šæ”¾ç½®ä½ç½®ï¼Œåˆ™ä¸æ·»åŠ æ–‡ä»¶è·¯å¾„æ³¨é‡Šã€‚
åœ¨ä»£ç å—ä¸­ï¼Œä½¿ç”¨'...existing code...'è¡Œæ³¨é‡Šæ¥æŒ‡ç¤ºæ–‡ä»¶ä¸­å·²å­˜åœ¨çš„ä»£ç ã€‚

ä»£ç å—ç¤ºä¾‹ï¼š

// filepath: /path/to/file
// ...existing code...
{ changed code }
// ...existing code...
{ changed code }
// ...existing code...


ç¡®ä¿è¡Œæ³¨é‡Šä½¿ç”¨æ­£ç¡®çš„ç¼–ç¨‹è¯­è¨€è¯­æ³•ï¼ˆä¾‹å¦‚Pythonç”¨"#"ã€Luaç”¨"--"ï¼‰ã€‚
å¯¹äºä»£ç å—ï¼Œä½¿ç”¨4ä¸ªåå¼•å·å¼€å§‹å’Œç»“æŸã€‚
é¿å…å°†æ•´ä¸ªå›ç­”ç”¨ä¸‰é‡åå¼•å·åŒ…è£¹ã€‚
é™¤éæ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™ä¸åŒ…å«å·®å¼‚æ ¼å¼ã€‚
ä¸è¦åœ¨ä»£ç å—ä¸­åŒ…å«è¡Œå·ã€‚

å½“ç»™å®šä»»åŠ¡æ—¶ï¼š

1. é€æ­¥æ€è€ƒï¼Œé™¤éç”¨æˆ·è¦æ±‚æˆ–ä»»åŠ¡éå¸¸ç®€å•ï¼Œå¦åˆ™ç”¨ä¼ªä»£ç æè¿°è®¡åˆ’
2. è¾“å‡ºä»£ç å—æ—¶ï¼Œç¡®ä¿åªåŒ…å«ç›¸å…³ä»£ç ï¼Œé¿å…é‡å¤æˆ–ä¸ç›¸å…³çš„ä»£ç 
3. ä»¥ç®€çŸ­çš„å»ºè®®ç»“æŸå›ç­”ï¼Œç›´æ¥æ”¯æŒç»§ç»­å¯¹è¯

é™„åŠ ä¸Šä¸‹æ–‡ï¼š
å½“å‰æ—¥æœŸæ˜¯%s
ç”¨æˆ·çš„Neovimç‰ˆæœ¬æ˜¯%s
ç”¨æˆ·æ­£åœ¨ä½¿ç”¨%sç³»ç»Ÿçš„æœºå™¨ä¸Šå·¥ä½œã€‚å¦‚æœé€‚ç”¨ï¼Œè¯·ä½¿ç”¨ç³»ç»Ÿç‰¹å®šçš„å‘½ä»¤è¿›è¡Œå“åº”ã€‚
]]

return -- lazy.nvim
{
    "olimorris/codecompanion.nvim",
    dependencies = {
        "nvim-lua/plenary.nvim",
        "nvim-treesitter/nvim-treesitter",
    },
    config = function()
        -- Other package managers
        require("codecompanion").setup({
            prompt_library = {
                ["Explain in chinese"] = {
                    strategy = "chat",
                    description = "ä¸­æ–‡è§£é‡Šä»£ç ",
                    opts = {
                        is_slash_cmd = true,
                        modes = { "v" },
                        short_name = "explain_in_chinese",
                        auto_submit = false,
                        user_prompt = false,
                        stop_context_insertion = true,
                        ignore_system_prompt = true,
                        -- adapter = {
                        --     name = "siliconflow_r1",
                        --     model = "deepseek-ai/DeepSeek-R1",
                        -- },
                    },
                    prompts = {
                        {
                            role = "system",
                            content = function(context)
                                local machine = vim.uv.os_uname().sysname
                                if machine == "Darwin" then
                                    machine = "Mac"
                                end
                                if machine:find("Windows") then
                                    machine = "Windows"
                                end
                                -- ä¸ºæ¯ç§ä¸åŒçš„aiå·¥å…·ç”Ÿæˆå¯¹åº”çš„ç³»ç»Ÿæç¤ºè¯
                                return string.format(
                                    default_prompt,
                                    os.date("%B %d, %Y"),
                                    vim.version().major .. "." .. vim.version().minor .. "." .. vim.version().patch,
                                    machine
                                )
                            end,
                            opts = {
                                visible = false,
                            },
                        },
                        {
                            role = "user",
                            content = function(context)
                                local input = require("codecompanion.helpers.actions").get_code(
                                    context.start_line,
                                    context.end_line
                                )

                                return string.format(
                                    [[è¯·è§£é‡Šbuffer %d ä¸­çš„è¿™æ®µä»£ç :<prompt>
```%s
%s
```
</prompt>
]],
                                    context.bufnr,
                                    context.filetype,
                                    input
                                )
                            end,
                            opts = {
                                contains_code = true,
                            },
                        },
                    },
                },
                ["Generate digest"] = {
                    strategy = "chat",
                    description = "å¯¹å½“å‰Bufferå†…å®¹ç”Ÿæˆæ‘˜è¦",
                    opts = {
                        modes = { "n", "v" },
                        short_name = "generate_digest",
                        auto_submit = true,
                        user_prompt = false,
                        stop_context_insertion = true,
                        is_slash_cmd = true,
                        -- Do not send default system prompt
                        ignore_system_prompt = true,
                        -- To customize the chat buffer UI, you can set a custom intro message:
                        intro_message = "Welcome to generate digest!",
                        adapter = {
                            name = "kimi_openai_online",
                            model = "moonshot-v1-128k",
                        },
                    },
                    prompts = {
                        {
                            role = "system",
                            content = function(context)
                                local machine = vim.uv.os_uname().sysname
                                if machine == "Darwin" then
                                    machine = "Mac"
                                end
                                if machine:find("Windows") then
                                    machine = "Windows"
                                end
                                return string.format(
                                    kimi_prompt,
                                    os.date("%B %d, %Y"),
                                    vim.version().major .. "." .. vim.version().minor .. "." .. vim.version().patch,
                                    machine
                                )
                            end,
                            opts = {
                                visible = false,
                            },
                        },
                        {
                            role = "user",
                            content = function(context)
                                local input = require("codecompanion.helpers.actions").get_code(
                                    context.start_line,
                                    context.end_line
                                )
                                -- if #input <= 0 or input == nil then
                                local content = vim.api.nvim_buf_get_lines(context.bufnr, 0, -1, false)
                                for i, line in ipairs(content) do
                                    input = input .. "\n" .. line
                                end
                                -- end

                                return string.format(
                                    [[è¯·å¯¹buffer %d ä¸­çš„å†…å®¹ç”Ÿæˆæ‘˜è¦:<prompt>%s</prompt>]],
                                    context.bufnr,
                                    input
                                )
                            end,
                            opts = {
                                contains_code = true,
                            },
                        },
                    },
                },
            },
            display = {
                action_palette = {
                    width = 100,
                    height = 0.45,
                    prompt = "Prompt ", -- Prompt used for interactive LLM calls
                    provider = "fzf_lua", -- Can be "default", "telescope", "fzf_lua", "mini_pick" or "snacks". If not specified, the plugin will autodetect installed providers.
                    opts = {
                        show_default_actions = true, -- Show the default actions in the action palette?
                        show_default_prompt_library = true, -- Show the default prompt library in the action palette?
                        title = "CodeCompanion actions", -- The title of the action palette
                    },
                },
                -- Inline display configuration options:
                --   layout: (string) The layout for the inline display. Available options: "vertical", "horizontal", "buffer".
                inline = {
                    layout = "vertical", -- vertical|horizontal|buffer
                },
                chat = {
                    auto_scroll = true,
                    opts = {
                        completion_provider = "blink", -- blink|cmp|coc|default
                        goto_file_action = "tabnew", -- this will always open the file in a new tab
                    },
                    -- Change the default icons
                    icons = {
                        buffer_pin = "ïµ ",
                        buffer_watch = "ğŸ‘€ ",
                    },

                    -- Alter the sizing of the debug window
                    debug_window = {
                        ---@return number|fun(): number
                        width = vim.o.columns - 5,
                        ---@return number|fun(): number
                        height = vim.o.lines - 2,
                    },

                    -- Options to customize the UI of the chat buffer
                    window = {
                        layout = "horizontal", -- float|vertical|horizontal|buffer
                        position = "bottom", -- left|right|top|bottom (nil will default depending on vim.opt.splitright|vim.opt.splitbelow)
                        border = "single",
                        height = 0.45,
                        width = 0.4,
                        relative = "editor",
                        full_height = true, -- when set to false, vsplit will be used to open the chat buffer vs. botright/topleft vsplit
                        sticky = false, -- when set to true and `layout` is not `"buffer"`, the chat buffer will remain opened when switching tabs
                        opts = {
                            breakindent = true,
                            cursorcolumn = false,
                            cursorline = false,
                            foldcolumn = "0",
                            linebreak = true,
                            list = false,
                            numberwidth = 1,
                            signcolumn = "no",
                            spell = false,
                            wrap = true,
                        },
                    },

                    ---Customize how tokens are displayed
                    ---@param tokens number
                    ---@param adapter CodeCompanion.Adapter
                    ---@return string
                    token_count = function(tokens, adapter)
                        return " (" .. tokens .. " tokens)"
                    end,
                },
            },
            strategies = {
                chat = {
                    -- adapter = "siliconflow_r1",
                    adapter = "qwen2_coder_local",
                    keymaps = {
                        close = {
                            modes = { n = "<C-q>", i = "<C-q>" },
                            opts = { silent = true, desc = "Close chat" },
                        },
                    },
                    variables = {
                        ["buffer"] = {
                            opts = {
                                default_params = "pin", -- or 'watch'
                            },
                        },
                    },
                    slash_commands = {
                        ["file"] = {
                            -- Location to the slash command in CodeCompanion
                            callback = "strategies.chat.slash_commands.file",
                            description = "Select a file using Telescope",
                            opts = {
                                provider = "telescope", -- Can be "default", "telescope", "fzf_lua", "mini_pick" or "snacks"
                                contains_code = true,
                            },
                        },
                    },
                    opts = {
                        ---Decorate the user message before it's sent to the LLM
                        ---@param message string
                        ---@param adapter CodeCompanion.Adapter
                        ---@param context table
                        ---@return string
                        prompt_decorator = function(message, adapter, context)
                            return string.format([[<prompt>%s</prompt>]], message)
                        end,
                        ---@param opts { adapter: CodeCompanion.HTTPAdapter, language: string }
                        ---@return string
                        system_prompt = function(opts)
                            local machine = vim.uv.os_uname().sysname
                            if machine == "Darwin" then
                                machine = "Mac"
                            end
                            if machine:find("Windows") then
                                machine = "Windows"
                            end
                            -- ä¸ºæ¯ç§ä¸åŒçš„aiå·¥å…·ç”Ÿæˆå¯¹åº”çš„ç³»ç»Ÿæç¤ºè¯
                            if opts.adapter == "kimi_openai_online" then
                                return string.format(
                                    kimi_prompt,
                                    os.date("%B %d, %Y"),
                                    vim.version().major .. "." .. vim.version().minor .. "." .. vim.version().patch,
                                    machine
                                )
                            else
                                return string.format(
                                    default_prompt,
                                    os.date("%B %d, %Y"),
                                    vim.version().major .. "." .. vim.version().minor .. "." .. vim.version().patch,
                                    machine
                                )
                            end
                        end,
                    },
                },
                inline = {
                    -- adapter = "siliconflow_r1",
                    adapter = "qwen2_coder_local",
                    keymaps = {
                        accept_change = {
                            modes = { n = "ga" },
                            description = "Accept the suggested change",
                        },
                        reject_change = {
                            modes = { n = "gr" },
                            opts = { nowait = true },
                            description = "Reject the suggested change",
                        },
                    },
                },
                agent = {
                    --adapter = "siliconflow_r1"
                    adapter = "qwen2_coder_local",
                },
            },
            opts = {
                log_level = "WARN", -- or "TRACE"
                language = "Chinese",
            },
            adapters = {
                acp = {
                    opts = {
                        show_defaults = false,
                    },
                    -- Define your custom adapters here
                },
                http = {
                    opts = {
                        show_defaults = false,
                        allow_insecure = true,
                    },
                    siliconflow_r1_deepseek_online = function()
                        return require("codecompanion.adapters").extend("deepseek", {
                            name = "siliconflow_r1_deepseek_online",
                            url = "https://api.siliconflow.cn/v1/chat/completions",
                            env = {
                                api_key = function()
                                    return os.getenv("DEEPSEEK_API_KEY_S")
                                end,
                            },
                            schema = {
                                model = {
                                    default = "deepseek-ai/DeepSeek-R1",
                                    choices = {
                                        ["deepseek-ai/DeepSeek-R1"] = { opts = { can_reason = false } },
                                        ["deepseek-ai/DeepSeek-V3"] = { opts = { can_reason = false } },
                                    },
                                },
                            },
                        })
                    end,
                    qwen2_coder_local = function()
                        return require("codecompanion.adapters").extend("ollama", {
                            name = "qwen2_coder_local",
                            url = "http://192.168.100.1:11434/api/chat", -- æœ¬åœ°æœåŠ¡åœ°å€
                            env = {
                                api_key = function()
                                    return nil
                                end, -- æœ¬åœ°æ¨¡å‹é€šå¸¸ä¸éœ€è¦ API key
                            },
                            headers = {
                                ["Content-Type"] = "application/json",
                            },
                            parameters = {
                                sync = true,
                            },
                            opts = {
                                vision = true,
                                thinking = false,
                                stream = true,
                            },
                            schema = {
                                model = {
                                    default = "qwen2.5-coder:0.5b", -- æ¨¡å‹åç§°éœ€ä¸æœåŠ¡ç«¯ä¸€è‡´
                                    choices = {
                                        ["qwen2.5-coder:0.5b"] = {
                                            opts = {
                                                can_reason = true, -- æ”¯æŒæ¨ç†ä»»åŠ¡
                                                -- max_tokens = 4096, -- æœ€å¤§ token æ•°
                                            },
                                        },
                                    },
                                },
                                num_ctx = {
                                    default = 16384,
                                },
                                think = {
                                    default = false,
                                },
                                keep_alive = {
                                    default = "5m",
                                },
                            },
                        })
                    end,
                    kimi_openai_online = function()
                        return require("codecompanion.adapters").extend("openai", {
                            name = "kimi_openai_online",
                            url = "https://api.moonshot.cn/v1/chat/completions",
                            env = {
                                api_key = function()
                                    return os.getenv("KIMI_API_KEY")
                                end,
                            },
                            headers = {
                                ["Content-Type"] = "application/json",
                                ["Authorization"] = "Bearer ${api_key}",
                            },
                            schema = {
                                model = {
                                    default = "moonshot-v1-128k",
                                    choices = {
                                        ["moonshot-v1-128k"] = { opts = { can_reason = true } },
                                    },
                                },
                            },
                        })
                    end,

                    qwen3_coder_plus_2025_online = function()
                        return require("codecompanion.adapters").extend("openai", {
                            name = "qwen3_coder_plus_2025_online",
                            url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
                            env = {
                                api_key = function()
                                    return os.getenv("QWEN3_CODER_PLUS_2025")
                                end,
                            },
                            schema = {
                                model = {
                                    default = "qwen3-coder-plus-2025-07-22",
                                    choices = {
                                        ["qwen3-coder-plus-2025-07-22"] = { opts = { can_reason = true } },
                                    },
                                    num_ctx = {
                                        default = 16384,
                                    },
                                    think = {
                                        default = false,
                                    },
                                    keep_alive = {
                                        default = "5m",
                                    },
                                },
                            },
                        })
                    end,
                    -- aliyun_deepseek = function()
                    --     return require("codecompanion.adapters").extend("deepseek", {
                    --         name = "aliyun_deepseek",
                    --         url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
                    --         env = {
                    --             api_key = function()
                    --                 return os.getenv("DEEPSEEK_API_ALIYUN")
                    --             end,
                    --         },
                    --         schema = {
                    --             model = {
                    --                 default = "deepseek-r1",
                    --                 choices = {
                    --                     ["deepseek-r1"] = { opts = { can_reason = true } },
                    --                 },
                    --             },
                    --         },
                    --     })
                    -- end,
                },
            },
        })
    end,
}
